# -*- coding: utf-8 -*-
"""DL Project Telecom_churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V83_z3QkymPppAyJO3VWrY0V4gVUCFeK
"""

#important libararies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
import warnings
warnings.filterwarnings 
import tensorflow as tf 
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler,LabelEncoder

#load dataset
df=pd.read_csv('telecom_churn.csv')
df.head()

# here we see target variable is churn and is a classification dataset 
# here we have to predict whether customer subscribe the service or not

# solve EDA
df.info()

# here we can see above 7043 rows and 21 columns and there is no any null values
# check null value
df.isna().sum()

# visulaize null value
sns.heatmap(df.isna())
plt.show()

# first we delte customerID 
df.drop('customerID',axis=1,inplace=True)

df.head()

# we got TotalCharges column have numeric value but its datatype is object lets check
df['TotalCharges'].unique()

df['TotalCharges'].value_counts()

#here we got space because of that its datatype is object now we replace that by mean value
df['TotalCharges']=df['TotalCharges'].replace(' ',np.nan)

df['TotalCharges'].isna().sum()

df['TotalCharges']=df['TotalCharges'].astype('float')

df.info()

df['TotalCharges'].fillna(df['TotalCharges'].mean(),inplace=True)

df['TotalCharges'].value_counts()

df.isna().sum()

df['Churn'].value_counts()

sns.countplot(data=df,x='Churn')
plt.show()

# first we select data separatly object type and numberic type then convert it into numberic type 
df_num=df.select_dtypes(['int64','float64'])
df_cat=df.select_dtypes('object')

df_cat.head()

df_num.head()

for col in df_cat:
  le=LabelEncoder()
  df_cat[col]=le.fit_transform(df_cat[col])

df_cat.head()

df_new=pd.concat([df_cat,df_num],axis=1)
df_new.head()

df_new.info()

x=df_new.drop('Churn',axis=1)
y=df_new['Churn']

x

y

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=1)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(xtrain, ytrain)
ypred = lr.predict(xtest)
print(classification_report(ytest,ypred))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

lr=LogisticRegression()
knn=KNeighborsClassifier()
svm=SVC()
svml=SVC(kernel="linear")
rt=RandomForestClassifier()

model=[]
model.append(lr)
model.append(knn)
model.append(svm)
#model.append(svml)
model.append(rt)

def mymodel(model):
  model.fit(xtrain,ytrain)
  ypred=model.predict(xtest)
  print(classification_report(ytest,ypred))

for i in model:
  print(i)
  mymodel(i)
  print()
  print()

#here we got best score on Logistic regression and random forest classifier 81% 
from imblearn.over_sampling import RandomOverSampler
ros=RandomOverSampler(random_state=1)
xsample,ysample=ros.fit_resample(xtrain,ytrain)

ysample.shape

#step :- initalise the model
ann=tf.keras.Sequential()
#step :- add the hidden layer(optional)
ann.add(tf.keras.layers.Dense(units=30,activation='relu')) # unit- is how many hidden box will be created
ann.add(tf.keras.layers.Dense(units=20,activation='relu'))
                                                     # relu - is rectified linear unit

 #step 2 :- add the output layer
ann.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
 #step 3:- estabilish connection between all the layer 
ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
# adam is nothing but advance of Gradient Decent
#step 4:- train the model     
ann.fit(xtrain,ytrain,batch_size=30,epochs=100)

from sklearn.metrics import classification_report

ypred=ann.predict(xtest)
ypred=ypred>0.5
print(classification_report(ytest,ypred))

#step :- initalise the model
ann=tf.keras.Sequential()
#step :- add the hidden layer(optional)
ann.add(tf.keras.layers.Dense(units=20,activation='relu')) # unit- is how many hidden box will be created
                                                         # relu - is rectified linear unit
ann.add(tf.keras.layers.Dense(units=15,activation='relu'))

 #step 2 :- add the output layer
ann.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
 #step 3:- estabilish connection between all the layer 
ann.compile(optimizer='adam',loss='binary_crossentropy')
# adam is nothing but advance of Gradient Decent
#step 4:- train the model     
ann.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=100)

lossdf=pd.DataFrame(ann.history.history)
lossdf.plot()

ann=tf.keras.Sequential()
ann.add(tf.keras.layers.Dense(units=30,activation='relu'))
ann.add(tf.keras.layers.Dense(units=20,activation='relu'))
ann.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))
ann.compile(optimizer='adam',loss='binary_crossentropy')

from tensorflow.keras.callbacks import EarlyStopping
earlystop = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=25)
ann.fit(xtrain, ytrain, epochs=600, validation_data=(xtest, ytest), callbacks=[earlystop])

lossdf=pd.DataFrame(ann.history.history)
lossdf.plot()

from tensorflow.keras.layers import Dropout
ann = tf.keras.Sequential()

ann.add(tf.keras.layers.Dense(units=30, activation="relu"))
ann.add(Dropout(rate=0.5))

ann.add(tf.keras.layers.Dense(units=15, activation="relu"))
ann.add(Dropout(rate=0.5))

ann.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

ann.compile(optimizer="adam", loss="binary_crossentropy")

ann.fit(xtrain, ytrain, epochs=600, validation_data=(xtest, ytest), callbacks=[earlystop])

lossdf=pd.DataFrame(ann.history.history)
lossdf.plot()

ypred=ann.predict(xtest)
ypred=ypred>0.5
print(classification_report(ytest,ypred))