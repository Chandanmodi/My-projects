# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EvzIWgSfA9IfOXy5rDB4huYK1krWTISC
"""

# important library
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/drive/MyDrive/creditcard.csv')
df.head()

df.info()

df.isnull().sum()

sns.heatmap(df.isnull())
plt.show()

X=df.drop('Class',axis=1)
Y=df['Class']

df['Class'].value_counts()

sns.countplot(data=df,x="Class")
c=df["Class"].value_counts()
plt.yticks(c)
plt.show()

# as we see there is data imbalncing so to balnce the dataset we have to use 
#sampling technique

# split the dataset
#split data into training and tetsing 70 and 30 percent
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=1)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,confusion_matrix

def create_model(model):
    model.fit(X_train,Y_train)
    Y_pred=model.predict(X_test)
    print('score',model.score(X_test,Y_test))
    print(classification_report(Y_test,Y_pred))
    print('fonfusion matrix')
    print(confusion_matrix(Y_test,Y_pred))
    return model

lr=LogisticRegression()
lr=create_model(lr)

# check data balancing of Y_train
pd.Series(Y_train).value_counts()

#we see there is much difference in yes and No data on training data manority is
# very less so can't use under sampling because if we use under sampling lost much 
# data and if we use under sampling we have to train model in less data its not 
# good so we use Over Sampling method 
from imblearn.over_sampling import RandomOverSampler
ros=RandomOverSampler()
X_train1,Y_train1=ros.fit_resample(X_train,Y_train)

# check after balancing of Y_train
pd.Series(Y_train1).value_counts()

# check data balancing of Y_test
pd.Series(Y_test).value_counts()

X_test1,Y_test1=ros.fit_resample(X_test,Y_test)

# check after balancing of Y_test
pd.Series(Y_test1).value_counts()

lr.fit(X_train1,Y_train1)
Y_pred=lr.predict(X_test1)
print(classification_report(Y_test1,Y_pred))
print(confusion_matrix(Y_test1,Y_pred))

# we got 84% recall after using sampling technique
# use Decision tree
from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(X_train1,Y_train1)
Y_pred=dt.predict(X_test1)
dt.score(X_test1,Y_test1)

# we got 85% score 
# now use purning technique
# two type of purning technique
# 1. max_depth 2. min_sample_leaf

def create_model(model):
    model.fit(X_train1,Y_train1)
    Y_pred=model.predict(X_test1)
    print('score',model.score(X_test1,Y_test1))
    print(classification_report(Y_test1,Y_pred))
    print('fonfusion matrix')
    print(confusion_matrix(Y_test1,Y_pred))
    return model

dt1=DecisionTreeClassifier(max_depth=8)# max_depth can't take more than 8
dt1=create_model(dt1)

# after using max_depth purning method increase recall from 85 to 82 but it is not also much good 
# try other one min sample leaf 
dt2=DecisionTreeClassifier(min_samples_leaf=45) # bydefault it is gini index
dt2=create_model(dt2)

#  now entropy method of max_depth
dt3=DecisionTreeClassifier(max_depth=8,criterion='entropy')# max_depth can't take more than 8
dt3=create_model(dt3)

dt4=DecisionTreeClassifier(min_samples_leaf=45,criterion='entropy') # min sample leaf of entropy
dt4=create_model(dt4)

# now we see the min_sample_leaf entropy method is not good but max_depth of both and min_sample leaf of gini have same
#score of recall
# now use Ensembpling technique
# here there of four types 
#1.Naive aggregation
# a. hard voting b. soft voting 
lr=LogisticRegression()
dt=DecisionTreeClassifier()
dt1=DecisionTreeClassifier(criterion='entropy')

model_list=[('logistic',lr),('dt gini',dt),('dt entropy',dt1)]
from sklearn.ensemble import VotingClassifier
hvc=VotingClassifier(estimators=model_list)# bydefault hard voting
hvc=create_model(hvc)

# we got 78% score of reacall and heigh score of FN and have to reduce it
svc=VotingClassifier(estimators=model_list,voting='soft') # soft voting
svc=create_model(hvc)

# now use 2nd method of ensembling technique
# 2 . bootstraping 
# a. bagging  b. pasting c. random forest tree
from sklearn.ensemble import BaggingClassifier
bc=BaggingClassifier(LogisticRegression(),n_estimators=10,max_samples=900,random_state=1)
bc=create_model(bc)                   # bagging method

pc=BaggingClassifier(lr,n_estimators=10,max_samples=780,random_state=1,bootstrap=False)# pasting method
pc=create_model(pc)

rft=BaggingClassifier(dt,n_estimators=10,max_samples=780,random_state=1)
rft=create_model(rft)    # randomforesttree

# we got maximum score with random forest tree :87% recall
# we got bad recall on bagging and pasting 
# now use  3rd method of ensempling technique stacking
from mlxtend.classifier import StackingClassifier

lr=LogisticRegression()
dt=DecisionTreeClassifier()
dt1=DecisionTreeClassifier(criterion='entropy')
model_list=[lr,dt,dt1]
meta=LogisticRegression()
sc=StackingClassifier(classifiers=model_list,meta_classifier=meta)
sc=create_model(sc)

# we got 79% recall on stacking it is good but bad on confusion matrix so go ahead
# now use 4th technique of ensempling 
#4 . boosting technique
# a. Ada boost b. gradient boost c. Extream gradient boost(XG boost)
# a ADA boost 
from sklearn.ensemble import AdaBoostClassifier
ada=AdaBoostClassifier(n_estimators=75)
ada=create_model(ada)

# now use gradient boosting 
from sklearn.ensemble import GradientBoostingClassifier
gbc=GradientBoostingClassifier(n_estimators=75)# n_estimators<=100
gbc=create_model(gbc)

#!pip install xgboost
from xgboost import XGBClassifier
xg=XGBClassifier(n_estimators=25,reg_alpha=1)# reg_alpa=1 means handle automatically overfitting
# reg means regularization
xg=create_model(xg)

# we got best score with randomforestTree 87% recall and also less score of confusion matrix FN
# Now use RandomForestTree separetly 
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(n_estimators=10,max_features=10,random_state=1)
rfc=create_model(rfc)

rfc.feature_importances_

X.columns

#SVM : Support vector machine : It is supervised learning classification algorithm
#There are different types of SVM : -
'''
1. Linear Separable data means Linear Kernal
2.  Non -Linear data
a. Polynomial Kernal function    
b. radial basis kernel function
'''

#1. Linear Separable data means Linear Kernal
from sklearn.svm import LinearSVC
#create the object of LinearSVC class
svc=LinearSVC(random_state=1)  #by default hard margin
#call function
svc=create_model(svc)

#here recall is 79 which is good but we can more better
#means can be possible ,model is overfit  why 
#perhaps , can be no error on training time but error on testing time , 
#what  do we do ,
#add some external error on training time  if create a object of 
#LinearSVC class 

#Soft margin means to reduced overfitting situation means some error add on
#training time 
#create object of LinearSVC class
#Soft Margin
svc=LinearSVC(random_state=1,C=0.05)
#here C is parameter which is used to
#add some error on training time set error 0.05 
#call function
svc=create_model(svc)

#recall=0.84 is very good but can be much better .
#can be possible , check either dataset is  linear or  dataset is non-linear  .
#use Non-linear kernal function of SVM means 2 classes are not separatable 
#with straight line 

#there are 2 types of Non-Linear Kernal function
#here 1. polynomial kernal function : increase the low dimension to 
#high dimension
#2. radial basis kernal function  both are non-linear data

#2.a  polynomial Kernel function 
#from sklearn.svm import SVC 
#SVC inbuilt class for non-linear data
#create the object of SVC class
#poly_svc=SVC(random_state=1,kernel="poly")
#call function
#poly_svc=create_model(poly_svc)

#will not be apply polynomial kernel function in bank dataset
#we use 3 . non-linear : radial basis kernel function
#create object of SVC class(for radial-basis)
#create the object of SVC class
#radial_svc=SVC(random_state=1,kernel="rbf")
#call function
#radial_svc=create_model(radial_svc)

# now use Future selection technique
# there are of 4 types
# 1. filter method
# a. correaltion coefficient b. Anova test c. chi_square test
# now on correlation coefficient it is used mostly in regression so now we use directly anova test
# Anova test
from sklearn.feature_selection import SelectKBest,f_regression
anova=SelectKBest(score_func=f_regression,k=6)# k means no. of inputs 
X_train_imp=anova.fit_transform(X_train1,Y_train1)
X_test_imp=anova.transform(X_test1)

anova.get_support()

X.columns

# imp column v3, v10, v12, v14, v16, v17
lr=LogisticRegression()
lr.fit(X_train_imp,Y_train1)
lr.score(X_test_imp,Y_test1)

# now check dataset have any negative value because chi2 not applied on negative value 
for col in X:
    print(df[col]<0)

# we got negative value so can't use chi2 
# now 2nd future selection method is wrapper method
# a. forward method b. backward method
# apply forwad method 
columns=[]
for col in X:
    columns.append(col)
    X_new=df[columns]
    X_train,X_test,Y_train,Y_test=train_test_split(X_new,Y,test_size=0.3,random_state=1)
    lr=LogisticRegression()
    lr.fit(X_train,Y_train)
    score=lr.score(X_test,Y_test)
    print('columns:- ',col,'score:- ',score)

#Backward Selection  : it is reverse of forward selection
# in it we take all input first and train the model then remove one by one from last 
#declare a empty column
columns=[]
X_new=X
n_col=X_new.shape[1] #no. of columns
#print(n_col)
for i in range(n_col,0,-1): 
    columns.append(X_new)
    X_train,X_test,y_train,y_test = train_test_split(X_new,Y,test_size=0.3,random_state=1)
     #create a object of LinearRegression
    lr=LogisticRegression()
    #we train the model
    lr.fit(X_train,y_train)
    #find the score
    score1=lr.score(X_test,y_test)
    print("Column : ",i, " Score : ",score1)
    X_new=X_new.iloc[:,:-1]  #iloc index location iloc[startrow:stoprow ,startcol:-1]
    #iloc[rowindex,colindex]
    print("After Remove Column : ", i)

# now next method is embedded method we can't use it on classification dataset because it is not 
#relevant for classification type data
# so we move to next one PCA method
#apply PCA : principal component analysis
#4. PCA means Principal Component Analysis
#Select input and output from dataset
X=df.drop("Class",axis=1)
Y=df["Class"]

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=1)
     #create a object of LogisticRegression

from sklearn.decomposition import PCA
#Create object of PCA
pc=PCA(n_components=10,random_state=1)#n_components : how many principal components

X_train_pc=pc.fit_transform(X_train,Y_train)
X_test_pc=pc.transform(X_test)

#create the object of LogisticRegression class
lr=LogisticRegression()

#train the model
lr.fit(X_train_pc,Y_train)
#check score
lr.score(X_test_pc,Y_test)